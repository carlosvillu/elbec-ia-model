{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Evaluation using API\n",
    "\n",
    "This notebook evaluates all normalized texts (_NOR.txt files) using the deployed evaluation API.\n",
    "\n",
    "For each folder (POS1, POS2, PRE), it:\n",
    "1. Reads the consignas CSV to get text metadata\n",
    "2. Processes each _NOR.txt file by sending it to the API\n",
    "3. Collects evaluation results (nota and feedback)\n",
    "4. Generates a CSV with results per folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# API Configuration\nAPI_HOST = \"https://your-runpod-instance.proxy.runpod.net\"  # Update this with your actual RunPod.io URL\nAPI_PORT = \"8000\"  # Default port\nAPI_BASE_URL = f\"{API_HOST}\" if API_HOST.startswith('http') else f\"http://{API_HOST}:{API_PORT}\"\n\n# Folders to process\nFOLDERS = ['POS1', 'POS2', 'PRE']\nDATA_DIR = 'data'\n\nprint(f\"API Base URL: {API_BASE_URL}\")\nprint(f\"Folders to process: {FOLDERS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_consignas_csv(folder_path: Path) -> pd.DataFrame:\n    \"\"\"\n    Load the consignas CSV file for a given folder.\n    Handles different column name variations (File ID vs FileID, Consigna vs TEXTpost2)\n    \"\"\"\n    csv_path = folder_path / 'consignas.csv'\n    if not csv_path.exists():\n        print(f\"Warning: {csv_path} not found\")\n        return pd.DataFrame()\n    \n    df = pd.read_csv(csv_path)\n    \n    # Normalize column names\n    if 'File ID' in df.columns:\n        df.rename(columns={'File ID': 'FileID'}, inplace=True)\n    if 'TEXTpost2' in df.columns:\n        df.rename(columns={'TEXTpost2': 'Consigna'}, inplace=True)\n    \n    return df\n\ndef get_nor_files(folder_path: Path) -> List[Path]:\n    \"\"\"\n    Get all _NOR.txt files in a folder, sorted by name.\n    \"\"\"\n    nor_files = sorted(folder_path.glob('*_NOR.txt'))\n    return nor_files\n\ndef extract_id_from_filename(filename: str) -> str:\n    \"\"\"\n    Extract the ID from a filename like 'POS1_11410001_NOR.txt' -> '11410001'\n    \"\"\"\n    match = re.search(r'_(\\d+)_NOR\\.txt$', filename)\n    if match:\n        return match.group(1)\n    return None\n\ndef extract_curso_from_id(text_id: str) -> str:\n    \"\"\"\n    Extract the curso (grade level) from the text ID.\n    The third character (index 2) of the ID represents the grade level.\n    \n    Examples:\n        '11410003' -> '4' -> '4t ESO'\n        '11510082' -> '5' -> '5è ESO'\n    \"\"\"\n    if not text_id or len(text_id) < 3:\n        return '4t ESO'  # Default fallback\n    \n    try:\n        curso_num = int(text_id[2])\n        \n        # Catalan ordinal mapping for ESO grades\n        curso_ordinals = {\n            1: '1r ESO',\n            2: '2n ESO',\n            3: '3r ESO',\n            4: '4t ESO',\n            5: '5è ESO',\n            6: '6è ESO'\n        }\n        \n        return curso_ordinals.get(curso_num, f'{curso_num}è ESO')\n    except (ValueError, IndexError):\n        return '4t ESO'  # Default fallback\n\ndef read_text_file(file_path: Path) -> str:\n    \"\"\"\n    Read and return the content of a text file.\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read().strip()\n    except Exception as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return \"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Interaction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_evaluation_job(items: List[Dict]) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Submit a batch of texts for evaluation.\n",
    "    Returns the job information including job_id and stream_url.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}/evaluate\"\n",
    "    payload = {\"items\": items}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error submitting evaluation job: {e}\")\n",
    "        return None\n",
    "\n",
    "def stream_results(job_id: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Stream results from the API using Server-Sent Events.\n",
    "    Returns a list of all evaluation results.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE_URL}/stream/{job_id}\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        buffer = \"\"\n",
    "        for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):\n",
    "            if chunk:\n",
    "                buffer += chunk\n",
    "                \n",
    "                # Process complete events\n",
    "                while '\\n\\n' in buffer:\n",
    "                    event_text, buffer = buffer.split('\\n\\n', 1)\n",
    "                    \n",
    "                    # Parse SSE format\n",
    "                    lines = event_text.strip().split('\\n')\n",
    "                    event_data = {}\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        if line.startswith('event:'):\n",
    "                            event_data['event'] = line[6:].strip()\n",
    "                        elif line.startswith('data:'):\n",
    "                            try:\n",
    "                                event_data['data'] = json.loads(line[5:].strip())\n",
    "                            except json.JSONDecodeError:\n",
    "                                pass\n",
    "                    \n",
    "                    # Handle different event types\n",
    "                    if event_data.get('event') == 'batch_complete':\n",
    "                        batch_results = event_data.get('data', {}).get('results', [])\n",
    "                        results.extend(batch_results)\n",
    "                        \n",
    "                        # Print progress\n",
    "                        progress = event_data.get('data', {}).get('progress', {})\n",
    "                        if progress:\n",
    "                            print(f\"Progress: {progress.get('completed', 0)}/{progress.get('total', 0)} \"\n",
    "                                  f\"({progress.get('percentage', 0):.1f}%)\")\n",
    "                    \n",
    "                    elif event_data.get('event') == 'complete':\n",
    "                        print(\"Job completed successfully\")\n",
    "                        break\n",
    "                    \n",
    "                    elif event_data.get('event') == 'error':\n",
    "                        error_msg = event_data.get('data', {}).get('message', 'Unknown error')\n",
    "                        print(f\"Error: {error_msg}\")\n",
    "                        break\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error streaming results: {e}\")\n",
    "        return results\n",
    "\n",
    "def evaluate_texts(items: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Submit texts for evaluation and wait for results.\n",
    "    \"\"\"\n",
    "    print(f\"Submitting {len(items)} texts for evaluation...\")\n",
    "    \n",
    "    # Submit job\n",
    "    job_info = submit_evaluation_job(items)\n",
    "    if not job_info:\n",
    "        print(\"Failed to submit evaluation job\")\n",
    "        return []\n",
    "    \n",
    "    job_id = job_info.get('job_id')\n",
    "    print(f\"Job submitted with ID: {job_id}\")\n",
    "    print(f\"Estimated time: {job_info.get('estimated_time_seconds', 0)} seconds\")\n",
    "    \n",
    "    # Stream results\n",
    "    print(\"Streaming results...\")\n",
    "    results = stream_results(job_id)\n",
    "    \n",
    "    print(f\"Received {len(results)} results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_folder(folder_name: str, batch_size: int = 10) -> pd.DataFrame:\n    \"\"\"\n    Process all _NOR.txt files in a folder.\n    Returns a DataFrame with evaluation results.\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Processing folder: {folder_name}\")\n    print(f\"{'='*80}\\n\")\n    \n    folder_path = Path(DATA_DIR) / folder_name\n    \n    # Load consignas CSV\n    consignas_df = load_consignas_csv(folder_path)\n    if consignas_df.empty:\n        print(f\"No consignas.csv found for {folder_name}\")\n        return pd.DataFrame()\n    \n    print(f\"Loaded {len(consignas_df)} consignas\")\n    \n    # Get all _NOR.txt files\n    nor_files = get_nor_files(folder_path)\n    print(f\"Found {len(nor_files)} _NOR.txt files\")\n    \n    if not nor_files:\n        print(f\"No _NOR.txt files found in {folder_name}\")\n        return pd.DataFrame()\n    \n    # Prepare results storage\n    all_results = []\n    \n    # Process files in batches\n    for i in range(0, len(nor_files), batch_size):\n        batch_files = nor_files[i:i+batch_size]\n        print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(nor_files)-1)//batch_size + 1} \"\n              f\"({len(batch_files)} files)...\")\n        \n        # Prepare batch items\n        batch_items = []\n        file_metadata = []  # Store metadata for matching results later\n        \n        for file_path in batch_files:\n            # Extract ID from filename\n            text_id = extract_id_from_filename(file_path.name)\n            if not text_id:\n                print(f\"Warning: Could not extract ID from {file_path.name}\")\n                continue\n            \n            # Extract curso from ID\n            curso = extract_curso_from_id(text_id)\n            \n            # Read text content\n            respuesta = read_text_file(file_path)\n            if not respuesta:\n                print(f\"Warning: Empty file {file_path.name}\")\n                continue\n            \n            # Get consigna from CSV\n            consigna_row = consignas_df[consignas_df['ID'].astype(str) == text_id]\n            if consigna_row.empty:\n                print(f\"Warning: No consigna found for ID {text_id}\")\n                consigna = \"N/A\"\n            else:\n                consigna = consigna_row.iloc[0]['Consigna']\n            \n            # Prepare API item\n            batch_items.append({\n                \"id_alumno\": text_id,\n                \"curso\": curso,\n                \"consigna\": consigna,\n                \"respuesta\": respuesta\n            })\n            \n            # Store metadata\n            file_metadata.append({\n                'id': text_id,\n                'filename': file_path.name,\n                'consigna': consigna,\n                'curso': curso\n            })\n        \n        if not batch_items:\n            print(\"No valid items in this batch, skipping...\")\n            continue\n        \n        # Evaluate batch\n        results = evaluate_texts(batch_items)\n        \n        # Match results with metadata\n        for result in results:\n            id_alumno = result.get('id_alumno')\n            metadata = next((m for m in file_metadata if m['id'] == id_alumno), None)\n            \n            if metadata:\n                all_results.append({\n                    'folder': folder_name,\n                    'id': id_alumno,\n                    'filename': metadata['filename'],\n                    'curso': metadata['curso'],\n                    'consigna': metadata['consigna'],\n                    'nota': result.get('nota'),\n                    'feedback': result.get('feedback')\n                })\n        \n        # Small delay between batches to avoid overwhelming the API\n        if i + batch_size < len(nor_files):\n            time.sleep(1)\n    \n    # Create DataFrame\n    results_df = pd.DataFrame(all_results)\n    \n    print(f\"\\n{'-'*80}\")\n    print(f\"Completed processing {folder_name}\")\n    print(f\"Total results: {len(results_df)}\")\n    if not results_df.empty:\n        print(f\"Average nota: {results_df['nota'].mean():.2f}\")\n        print(f\"Nota range: {results_df['nota'].min():.0f} - {results_df['nota'].max():.0f}\")\n    print(f\"{'-'*80}\\n\")\n    \n    return results_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API health before starting\n",
    "try:\n",
    "    health_url = f\"{API_BASE_URL}/health\"\n",
    "    response = requests.get(health_url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    health_data = response.json()\n",
    "    print(f\"API Health Check: {health_data.get('status', 'unknown')}\")\n",
    "    print(f\"Model loaded: {health_data.get('model_loaded', False)}\")\n",
    "    print(f\"GPU available: {health_data.get('gpu_available', False)}\")\n",
    "    print(\"\\nAPI is ready!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not check API health: {e}\")\n",
    "    print(\"Continuing anyway...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each folder and save results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for folder_name in FOLDERS:\n",
    "    try:\n",
    "        # Process folder\n",
    "        results_df = process_folder(folder_name, batch_size=10)\n",
    "        \n",
    "        if results_df.empty:\n",
    "            print(f\"No results for {folder_name}, skipping CSV generation\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_filename = f\"results_{folder_name}_{timestamp}.csv\"\n",
    "        output_path = Path(DATA_DIR) / folder_name / output_filename\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✓ Results saved to: {output_path}\\n\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(f\"Summary for {folder_name}:\")\n",
    "        print(results_df['nota'].describe())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {folder_name}: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All folders processed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Combine All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results into a single DataFrame\n",
    "all_results = []\n",
    "\n",
    "for folder_name in FOLDERS:\n",
    "    folder_path = Path(DATA_DIR) / folder_name\n",
    "    csv_files = sorted(folder_path.glob(f\"results_{folder_name}_*.csv\"))\n",
    "    \n",
    "    if csv_files:\n",
    "        # Get the most recent file\n",
    "        latest_csv = csv_files[-1]\n",
    "        df = pd.read_csv(latest_csv)\n",
    "        all_results.append(df)\n",
    "        print(f\"Loaded {len(df)} results from {latest_csv.name}\")\n",
    "\n",
    "if all_results:\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    combined_output = f\"results_all_folders_{timestamp}.csv\"\n",
    "    combined_df.to_csv(combined_output, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n✓ Combined results saved to: {combined_output}\")\n",
    "    print(f\"\\nTotal results: {len(combined_df)}\")\n",
    "    print(f\"\\nOverall statistics:\")\n",
    "    print(combined_df.groupby('folder')['nota'].describe())\n",
    "else:\n",
    "    print(\"No results files found to combine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}